{
  "id": "bookscorpus",
  "name": {
    "en": "BooksCorpus",
    "zh": "图书语料库"
  },
  "organization": {
    "name": "University of Toronto and MIT",
    "type": "research_institution",
    "country": null,
    "website": "https://www.mit.edu"
  },
  "description": {
    "en": "BooksCorpus is a large-scale text corpus containing over 11,000 unpublished books from various genres. Created by researchers at the University of Toronto and MIT, it has become one of the most influential datasets in natural language processing. The corpus was instrumental in training breakthrough language models like BERT, GPT, and other transformer-based architectures. It provides diverse, narrative text covering fiction, non-fiction, and multiple writing styles, making it ideal for unsupervised pre-training of language models.",
    "zh": "图书语料库(BooksCorpus)是一个大规模文本语料库，包含超过11,000本来自不同类型的未出版书籍。由多伦多大学和麻省理工学院的研究人员创建，已成为自然语言处理领域最具影响力的数据集之一。该语料库在训练BERT、GPT等突破性语言模型和其他基于Transformer的架构中发挥了关键作用。它提供了涵盖小说、非小说和多种写作风格的丰富叙事文本，非常适合用于语言模型的无监督预训练。"
  },
  "access": {
    "primary_url": "https://arxiv.org/abs/1506.06724",
    "api": {
      "available": false
    },
    "download": {
      "available": true,
      "formats": ["TXT"]
    },
    "access_level": "open",
    "registration_required": false
  },
  "coverage": {
    "geographic": {
      "scope": "global"
    },
    "temporal": {
      "start_year": 2015,
      "end_year": 2015,
      "update_frequency": "one-time"
    },
    "domains": [
      "Natural Language Processing",
      "Machine Learning",
      "Computational Linguistics",
      "Text Mining",
      "Deep Learning"
    ],
    "indicators": 11000
  },
  "data_content": {
    "en": [
      "Over 11,000 unpublished books from Smashwords",
      "984.5 million words total",
      "74 million sentences",
      "Fiction and non-fiction genres",
      "Adventure, fantasy, romance, science fiction",
      "Historical, thriller, young adult",
      "Business, biography, self-help",
      "Continuous narrative text",
      "Diverse writing styles and vocabularies",
      "English language only"
    ],
    "zh": [
      "超过11,000本来自Smashwords的未出版书籍",
      "总计9.845亿个单词",
      "7400万个句子",
      "小说和非小说类型",
      "冒险、奇幻、浪漫、科幻",
      "历史、惊悚、青少年读物",
      "商业、传记、自助",
      "连续的叙事文本",
      "多样的写作风格和词汇",
      "仅限英语"
    ]
  },
  "data_characteristics": {
    "types": ["text"],
    "granularity": ["sentence-level", "document-level"],
    "formats": ["TXT", "Parquet", "Arrow"],
    "languages": ["en"]
  },
  "quality": {
    "authority_level": 3,
    "methodology_transparency": 3,
    "update_timeliness": 2,
    "data_completeness": 4,
    "documentation_quality": 3,
    "citation_count": 5
  },
  "licensing": {
    "license": "Free for research purposes",
    "commercial_use": false,
    "attribution_required": true,
    "restrictions": [
      "Original dataset no longer officially maintained",
      "Licensing status unclear for some books",
      "Not recommended for commercial applications",
      "Some books may have been removed due to copyright concerns"
    ]
  },
  "metadata": {
    "data_dictionary": true,
    "methodology_docs": true,
    "user_guide": true
  },
  "usage": {
    "use_cases": [
      "Pre-training language models (BERT, GPT, RoBERTa)",
      "Unsupervised representation learning",
      "Text generation model training",
      "Language understanding benchmarks",
      "Transfer learning in NLP",
      "Long-form text modeling",
      "Narrative understanding research",
      "Writing style analysis"
    ],
    "example_studies": [
      "BERT: Pre-training of Deep Bidirectional Transformers (Google AI, 2018)",
      "Improving Language Understanding by Generative Pre-Training (OpenAI GPT, 2018)",
      "RoBERTa: A Robustly Optimized BERT Pretraining Approach (Facebook AI, 2019)"
    ]
  },
  "related_sources": [
    "common-crawl",
    "wikipedia",
    "openwebtext"
  ],
  "contact": {
    "support_url": "https://www.mit.edu"
  },
  "catalog_metadata": {
    "added_date": "2025-12-11",
    "last_updated": "2025-12-11",
    "verified_date": "2025-12-11",
    "contributor": "Claude (datasource-fetcher)",
    "status": "active"
  },
  "tags": [
    "natural language processing",
    "NLP",
    "text corpus",
    "language modeling",
    "BERT",
    "GPT",
    "transformer",
    "pre-training",
    "unsupervised learning",
    "books",
    "narrative text",
    "machine learning",
    "deep learning",
    "computational linguistics"
  ]
}
