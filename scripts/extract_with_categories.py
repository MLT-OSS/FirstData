#!/usr/bin/env python3
import re
from pathlib import Path
from collections import defaultdict

# å­ç±»åˆ«æ˜ å°„è§„åˆ™
SUBCATEGORY_MAPPING = {
    # International
    'ç»æµ': 'economics',
    'Economics': 'economics',
    'è´¸æ˜“': 'trade',
    'Trade': 'trade',
    'èƒ½æº': 'energy',
    'Energy': 'energy',
    'å¥åº·': 'health',
    'Health': 'health',
    'ç¯å¢ƒ': 'environment',
    'Environment': 'environment',
    'å†œä¸š': 'agriculture',
    'Agriculture': 'agriculture',
    'å‘å±•': 'development',
    'Development': 'development',
    'å‘å±•é‡‘è': 'development-finance',
    'Development Finance': 'development-finance',
    'æ•™è‚²': 'education',
    'Education': 'education',
    'é‡‘è': 'finance',
    'Finance': 'finance',
    'çŸ¥è¯†äº§æƒ': 'intellectual-property',
    'Intellectual Property': 'intellectual-property',
    'åœ°çƒç§‘å­¦': 'earth-science',
    'Earth Science': 'earth-science',
    'ç”Ÿç‰©å­¦': 'biology',
    'Biology': 'biology',
    'åŒ–å­¦': 'chemistry',
    'Chemistry': 'chemistry',
    'åŠ³å·¥ä¸ç¤¾ä¼š': 'labour-social',
    'Labour & Social': 'labour-social',
    'äº¤é€šè¿è¾“': 'transport',
    'Transport': 'transport',
    'æ ‡å‡†ä¸è®¡é‡': 'standards',
    'Standards & Metrology': 'standards',

    # Countries (regions)
    'åŒ—ç¾æ´²': 'north-america',
    'æ¬§æ´²': 'europe',
    'äºšæ´²': 'asia',
    'å¤§æ´‹æ´²': 'oceania',
    'å—ç¾æ´²': 'south-america',
    'éæ´²': 'africa',

    # Specific countries to regions
    'ç¾å›½': 'north-america',
    'åŠ æ‹¿å¤§': 'north-america',
    'å¢¨è¥¿å“¥': 'north-america',
    'æ—¥æœ¬': 'asia',
    'éŸ©å›½': 'asia',
    'æ–°åŠ å¡': 'asia',
    'å°åº¦': 'asia',
    'è‹±å›½': 'europe',
    'å¾·å›½': 'europe',
    'æ³•å›½': 'europe',
    'æ¬§ç›Ÿ': 'europe',
    'æ¾³å¤§åˆ©äºš': 'oceania',
    'æ–°è¥¿å…°': 'oceania',
    'å·´è¥¿': 'south-america',
    'é˜¿æ ¹å»·': 'south-america',
    'æ™ºåˆ©': 'south-america',
    'å“¥ä¼¦æ¯”äºš': 'south-america',

    # Academic
    'ç»¼åˆæ€§æ•°æ®ä»“åº“': 'repositories',
    'ç»æµå­¦': 'economics',
    'å¥åº·åŒ»å­¦': 'health',
    'ç¯å¢ƒç§‘å­¦': 'environment',
    'ç¤¾ä¼šç§‘å­¦': 'social',
    'ç‰©ç†åŒ–å­¦': 'physics_chemistry',
    'ç”Ÿå‘½ç§‘å­¦': 'biology',

    # Sectors
    'èƒ½æºé¢†åŸŸ': 'energy',
    'èƒ½æº': 'energy',
    'ç§‘æŠ€åˆ›æ–°': 'innovation_patents',
    'ä¸“åˆ©': 'innovation_patents',
    'æ•™è‚²è¯„ä¼°': 'education',
    'å†œä¸šä¸é£Ÿå“': 'agriculture_food',
    'å†œä¸š': 'agriculture_food',
    'é‡‘èå¸‚åœº': 'finance_markets',
    'è®¡ç®—æœºç§‘å­¦ä¸AI': 'computer_science_ai',
    'AI/ML': 'computer_science_ai',
    'ML': 'computer_science_ai',
    'è‡ªç„¶è¯­è¨€å¤„ç†': 'nlp',
    'åœ°çƒç§‘å­¦ä¸åœ°ç†ä¿¡æ¯': 'geoscience_geography',
    'åœ°çƒç§‘å­¦ä¸åœ°ç†': 'geoscience_geography',
    'ç”Ÿç‰©ä¸ç”Ÿå‘½ç§‘å­¦': 'biology',
    'åŒ–å­¦ä¸ææ–™': 'chemistry_materials',
    'ç¤¾äº¤åª’ä½“ä¸ç½‘ç»œæ•°æ®': 'social_media',
    'ç¤¾äº¤åª’ä½“ä¸ç½‘ç»œ': 'social_media',
    'ä½“è‚²è¿åŠ¨': 'sports',
    'äº¤é€šè¿è¾“': 'transportation',
    'åšç‰©é¦†ä¸æ–‡åŒ–é—äº§': 'museums_culture',
    'åšç‰©é¦†ä¸æ–‡åŒ–': 'museums_culture',
    'æ—¶é—´åºåˆ—æ•°æ®': 'timeseries',
    'ç½‘ç»œå®‰å…¨': 'cybersecurity',
    'å…¶ä»–ä¸“ä¸šé¢†åŸŸ': 'other',
    'å…¶ä»–é¢†åŸŸ': 'other',
}

def extract_datasource_full_name(line):
    """Extract full datasource name including Chinese description"""
    line = re.sub(r'^[0-9]+\.\s*', '', line)
    line = re.sub(r'^-\s*', '', line)
    line = re.sub(r'ğŸ“‹\s*', '', line)
    name = re.sub(r'\s*â­\s*', '', line)
    name = re.sub(r'\s*ğŸ’\s*', '', name)
    name = re.sub(r'\s*ï¼ˆ[^ï¼‰]+ï¼‰\s*$', '', name)
    return name.strip()

def normalize_for_dedup(name):
    """Normalize a datasource name for deduplication comparison"""
    main_part = name.split('-')[0].strip()
    normalized = main_part.lower().strip()
    return normalized

def clean_section_name(section):
    """Clean section name by removing extra markers and text"""
    if not section:
        return section

    # Remove emoji
    section = re.sub(r'[\U0001F1E0-\U0001F1FF]+\s*', '', section)  # Flags
    section = re.sub(r'[â­ğŸ’ğŸ“‹âœ…ğŸ”¶ğŸ”·]+', '', section)  # Other emoji

    # Remove patterns like (Xhä¸ª), (Xä¸ªå¾…å®Œæˆ), - å·²å®Œæˆ X/X
    section = re.sub(r'\s*[ï¼ˆ\(]\d+[ä¸ªh][^)ï¼‰]*[ï¼‰\)]', '', section)
    section = re.sub(r'\s*-\s*å·²å®Œæˆ\s*\d+/\d+', '', section)
    section = re.sub(r'\s*-\s*å·²å®Œæˆ', '', section)

    return section.strip()

def extract_section_title(line):
    """Extract section title from markdown header"""
    # Match headers like: ### ç»æµ | Economics, #### èƒ½æºé¢†åŸŸ, ### ğŸ‡ºğŸ‡¸ ç¾å›½
    line = line.strip()
    if line.startswith('#'):
        # Remove # symbols
        title = re.sub(r'^#+\s*', '', line)
        # Remove emoji flags
        title = re.sub(r'[\U0001F1E0-\U0001F1FF]+\s*', '', title)
        # Extract text before |
        if '|' in title:
            parts = title.split('|')
            cn_part = clean_section_name(parts[0].strip())
            en_part = clean_section_name(parts[1].strip()) if len(parts) > 1 else ''
            return cn_part, en_part
        cleaned = clean_section_name(title.strip())
        return cleaned, ''
    return None, None

def get_main_category(filename):
    """Get main category from filename"""
    name = Path(filename).stem
    if name == 'international':
        return 'international', 'å›½é™…ç»„ç»‡'
    elif name == 'countries':
        return 'countries', 'å„å›½å®˜æ–¹'
    elif name == 'academic':
        return 'academic', 'å­¦æœ¯ç ”ç©¶'
    elif name == 'sectors':
        return 'sectors', 'è¡Œä¸šé¢†åŸŸ'
    return None, None

def infer_country_region_from_name(datasource_name):
    """Infer country/region from datasource name"""
    name_lower = datasource_name.lower()

    # Oceania (check first to avoid conflicts with "bureau")
    if any(x in name_lower for x in ['australia', 'australian', 'data.gov.au', 'geoscience australia']):
        return 'oceania'
    if any(x in name_lower for x in ['new zealand', 'stats nz', 'data.govt.nz']):
        return 'oceania'

    # North America
    if any(x in name_lower for x in ['canada', 'canadian']):
        return 'north-america'
    if any(x in name_lower for x in ['mexico', 'mexican', 'inegi', 'coneval', 'semarnat', 'datos.gob.mx']):
        return 'north-america'
    # US-specific terms (check for Australia first to avoid false positives)
    if any(x in name_lower for x in ['bureau of labor', 'bureau of economic', 'bureau of meteorology']):
        # Bureau of Meteorology is Australian, but others are US
        if 'meteorology' in name_lower:
            return 'oceania'
        return 'north-america'
    if any(x in name_lower for x in ['united states', 'u.s.', 'us ', 'eia', 'epa', 'cdc',
                                      'sec ', 'uspto', 'nces', 'fred', 'data.gov', 'usda', 'nasa earth']):
        return 'north-america'

    # Europe
    if any(x in name_lower for x in ['european', 'eurostat', 'europeana']):
        return 'europe'
    if any(x in name_lower for x in ['england', 'uk ', 'british', 'data.gov.uk', 'nhs']):
        return 'europe'
    if any(x in name_lower for x in ['germany', 'german', 'france', 'french']):
        return 'europe'

    # Asia
    if any(x in name_lower for x in ['japan', 'japanese', 'e-stat', 'ministry of finance', 'ministry of economy']):
        # Check if it's specifically Japanese context
        if 'ministry' in name_lower:
            return 'asia'
        return 'asia'
    if any(x in name_lower for x in ['korea', 'korean', 'data.go.kr']):
        return 'asia'
    if any(x in name_lower for x in ['singapore', 'singstat', 'data.gov.sg', 'monetary authority of singapore']):
        return 'asia'
    if any(x in name_lower for x in ['india', 'indian', 'data.gov.in', 'niti aayog']):
        return 'asia'

    # South America
    if any(x in name_lower for x in ['brazil', 'brazilian', 'ibge', 'dados.gov.br']):
        return 'south-america'
    if any(x in name_lower for x in ['argentina', 'argentinian']):
        return 'south-america'
    if any(x in name_lower for x in ['chile', 'chilean']):
        return 'south-america'
    if any(x in name_lower for x in ['colombia', 'colombian']):
        return 'south-america'

    return None

def map_subcategory(section_cn, section_en, main_category, datasource_name=''):
    """Map section title to subcategory path"""
    # For countries, try to infer from datasource name first
    if main_category == 'countries':
        inferred_region = infer_country_region_from_name(datasource_name)
        if inferred_region:
            return inferred_region

    # Try Chinese first
    if section_cn in SUBCATEGORY_MAPPING:
        return SUBCATEGORY_MAPPING[section_cn]
    # Try English
    if section_en in SUBCATEGORY_MAPPING:
        return SUBCATEGORY_MAPPING[section_en]

    # Partial matching for compound titles
    for key, value in SUBCATEGORY_MAPPING.items():
        if key in section_cn or key in section_en:
            return value

    return 'other'

def find_datasources_with_categories():
    """Find all datasources with their categories"""
    datasources_dict = {}  # normalized_name -> (full_name, main_cat, sub_cat, cn_name, en_name)
    tasks_dir = Path('/Users/mlamp/project/datasource-hub/tasks')

    invalid_patterns = [
        r'^\*\*', r'^é€‰æ‹©ä»»åŠ¡', r'^çŠ¶æ€', r'^å¾…', r'^å·²',
        r'^è¯¦ç»†æ¸…å•', r'^\d+ä¸ª$', r'^[ï¼ˆ\(]', r'^æ›´æ–°', r'â†’',
    ]

    # Process main task files
    for md_file in [tasks_dir / 'international.md', tasks_dir / 'countries.md',
                     tasks_dir / 'academic.md', tasks_dir / 'sectors.md']:
        if not md_file.exists():
            continue

        main_cat, main_cat_cn = get_main_category(md_file)
        if not main_cat:
            continue

        current_section_cn = ''
        current_section_en = ''

        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                for line in f:
                    # Check for section headers
                    section_cn, section_en = extract_section_title(line)
                    if section_cn:
                        current_section_cn = section_cn
                        current_section_en = section_en
                        continue

                    # Check for datasources
                    if 'ğŸ“‹' in line and (line.strip().startswith('-') or re.match(r'^\d+\.', line.strip())):
                        if 'å¾…å®Œæˆ' in line or 'å¾…å¼€å§‹' in line:
                            continue

                        full_name = extract_datasource_full_name(line)

                        # Skip invalid entries
                        skip = False
                        for pattern in invalid_patterns:
                            if re.search(pattern, full_name):
                                skip = True
                                break

                        if not skip and full_name and len(full_name) > 3:
                            norm_key = normalize_for_dedup(full_name)
                            sub_cat = map_subcategory(current_section_cn, current_section_en, main_cat, full_name)

                            # Keep the longer/more detailed version
                            if norm_key not in datasources_dict or len(full_name) > len(datasources_dict[norm_key][0]):
                                datasources_dict[norm_key] = (
                                    full_name,
                                    main_cat,
                                    sub_cat,
                                    main_cat_cn,
                                    current_section_cn
                                )

        except Exception as e:
            print(f"Error processing {md_file}: {e}")

    # Sort by datasource name
    sorted_items = sorted(datasources_dict.values(), key=lambda x: x[0])
    return sorted_items

if __name__ == '__main__':
    datasources = find_datasources_with_categories()

    # Write to file with categories
    output_file = '/Users/mlamp/project/datasource-hub/batch-datasource.txt'
    with open(output_file, 'w', encoding='utf-8') as f:
        for full_name, main_cat, sub_cat, main_cat_cn, section_cn in datasources:
            # Format: Datasource Name | main/sub | ä¸»ç±»åˆ«/å­ç±»åˆ«
            category_path = f"{main_cat}/{sub_cat}"
            # Clean the Chinese category name
            section_cn_clean = clean_section_name(section_cn)
            category_cn = f"{main_cat_cn}/{section_cn_clean}" if section_cn_clean else main_cat_cn
            f.write(f"{full_name} | {category_path} | {category_cn}\n")

    print(f"âœ… Found {len(datasources)} datasources with categories")
    print(f"âœ… Written to {output_file}")
    print("\n=== å‰15ä¸ªæ•°æ®æºï¼ˆå¸¦ç±»åˆ«ï¼‰ ===")
    for i, (name, main, sub, main_cn, sec_cn) in enumerate(datasources[:15], 1):
        cat_path = f"{main}/{sub}"
        cat_cn = f"{main_cn}/{sec_cn}" if sec_cn else main_cn
        print(f"{i}. {name}")
        print(f"   ğŸ“ {cat_path} | {cat_cn}")

    print("\n=== ç±»åˆ«ç»Ÿè®¡ ===")
    cat_stats = defaultdict(int)
    for _, main_cat, sub_cat, _, _ in datasources:
        cat_stats[f"{main_cat}/{sub_cat}"] += 1

    for cat, count in sorted(cat_stats.items(), key=lambda x: -x[1])[:20]:
        print(f"{cat}: {count}")
